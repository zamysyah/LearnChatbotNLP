# -*- coding: utf-8 -*-
"""Chatbot3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ClR1eWaYjuFYbVPsZMIJzMMrwNoN943D
"""

import pandas as pd
import random
import nltk
from nltk.corpus import wordnet
from tqdm import tqdm

# Download resource WordNet
nltk.download("wordnet")
nltk.download("omw-1.4")

# Load dataset (ganti path sesuai kebutuhan kamu)
df = pd.read_csv("intent_dataset.csv")  # tanpa /content kalau file di root

def synonym_augment(sentence, n=2):
    """Ganti sampai n kata dalam kalimat dengan sinonim dari WordNet."""
    words = sentence.split()
    new_words = words[:]
    random_word_list = list(set([word for word in words if wordnet.synsets(word)]))
    random.shuffle(random_word_list)

    num_replaced = 0
    for word in random_word_list:
        synonyms = wordnet.synsets(word)
        if not synonyms:
            continue
        synonym_words = set()
        for syn in synonyms:
            for lemma in syn.lemmas():
                synonym_words.add(lemma.name().replace("_", " "))
        synonym_words.discard(word)
        if synonym_words:
            new_word = random.choice(list(synonym_words))
            new_words = [new_word if w == word else w for w in new_words]
            num_replaced += 1
        if num_replaced >= n:
            break

    return " ".join(new_words)

# Buat data augmentasi
augmented_data = []
for _, row in tqdm(df.iterrows(), total=len(df)):
    for _ in range(2):  # 2 variasi per pattern
        new_pattern = synonym_augment(row["patterns"])
        augmented_data.append({
            "patterns": new_pattern,
            "tag": row["tag"],
            "responses": row["responses"]
        })

# Gabungkan dengan data asli
aug_df = pd.concat([df, pd.DataFrame(augmented_data)], ignore_index=True)

# Simpan ke CSV
aug_df.to_csv("/content/augmented_intent_dataset.csv", index=False)
print("✅ Dataset augmentasi berhasil disimpan ke 'augmented_intent_dataset.csv'")

"""**tahapan evaluasi**"""

import pandas as pd

# Baca dataset
df = pd.read_csv("/content/augmented_intent_dataset.csv")

# Hitung frekuensi tiap label/tag
label_counts = df['tag'].value_counts()

# Tentukan ambang batas label minor
threshold = 3
minor_labels = label_counts[label_counts < threshold].index.tolist()

# Tampilkan label minor (opsional)
print("Label minor yang akan digabung:", minor_labels)

# Gabungkan label minor ke dalam satu label umum, misalnya 'other'
df['tag'] = df['tag'].apply(lambda x: 'other' if x in minor_labels else x)

# Simpan dataset hasil penggabungan
df.to_csv("/content/merged_intent_dataset.csv", index=False)
print("✅ Dataset dengan label minor digabung berhasil disimpan sebagai 'merged_intent_dataset.csv'")

from sklearn.metrics import classification_report
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import LabelEncoder

# 1. Baca dataset
df = pd.read_csv('/content/merged_intent_dataset.csv')

# 2. Preprocessing sederhana
df['patterns'] = df['patterns'].str.lower()

# 3. Encode label
le = LabelEncoder()
df['encoded_tag'] = le.fit_transform(df['tag'])

# 4. Split data
X_train, X_test, y_train, y_test = train_test_split(df['patterns'], df['encoded_tag'], test_size=0.2, random_state=42, stratify=df['encoded_tag'])

# 5. Vectorization
vectorizer = TfidfVectorizer()
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

# 6. Training
model = LogisticRegression(max_iter=1000)
model.fit(X_train_vec, y_train)

# Prediksi
y_pred = model.predict(X_test_vec)

# Evaluasi
print(f"📊 Accuracy: {accuracy_score(y_test, y_pred):.2f}")

# Hanya ambil label yang ada di y_test
unique_test_labels = np.unique(y_test)
target_names = le.inverse_transform(unique_test_labels)

print("📋 Classification Report:")
print(classification_report(y_test, y_pred, labels=unique_test_labels, target_names=target_names))


# Simpan model dan encoder
import pickle
with open("intent_model.pkl", "wb") as f:
    pickle.dump(model, f)
with open("vectorizer.pkl", "wb") as f:
    pickle.dump(vectorizer, f)
with open("label_encoder.pkl", "wb") as f:
    pickle.dump(le, f)

print("✅ Model, vectorizer, dan encoder berhasil disimpan.")

"""**EVALUASI KEDUA**"""

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

# Param grid
param_grid_lr = {
    'C': [0.1, 1, 10],
    'solver': ['liblinear', 'lbfgs'],
    'penalty': ['l2']
}

# Grid search
grid_lr = GridSearchCV(LogisticRegression(max_iter=1000), param_grid_lr, cv=5, scoring='accuracy', n_jobs=-1)
grid_lr.fit(X_train_vec, y_train)

# Best model
best_lr = grid_lr.best_estimator_
y_pred_best_lr = best_lr.predict(X_test_vec)

print("📊 Best Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_best_lr))
print("📋 Classification Report:")
print(classification_report(y_test, y_pred_best_lr, target_names=le.inverse_transform(np.unique(y_test))))

import pickle
import random

# Load model, vectorizer, dan encoder
with open("intent_model.pkl", "rb") as f:
    model = pickle.load(f)

with open("vectorizer.pkl", "rb") as f:
    vectorizer = pickle.load(f)

with open("label_encoder.pkl", "rb") as f:
    le = pickle.load(f)

# Load intent-response mapping
import pandas as pd
df = pd.read_csv("merged_intent_dataset.csv")
intent_response_map = df.groupby("tag")["responses"].apply(list).to_dict()

# Fungsi chatbot
def chatbot_response(text):
    text_vec = vectorizer.transform([text.lower()])
    pred = model.predict(text_vec)[0]
    intent = le.inverse_transform([pred])[0]
    responses = intent_response_map.get(intent, ["Maaf, saya tidak mengerti."])
    return random.choice(responses)

# Interaksi
if __name__ == "__main__":
    print("🧠 Chatbot aktif. Ketik 'keluar' untuk berhenti.")
    while True:
        user_input = input("Kamu: ")
        if user_input.lower() == "keluar":
            print("Chatbot: Sampai jumpa!")
            break
        response = chatbot_response(user_input)
        print("Chatbot:", response)

# chatbot.py
import random
import datetime
import requests
import pickle
import json

API_KEY = "b46e6fad8459cedb9b6d6bc6b40bc4d4"
BASE_URL = "http://api.openweathermap.org/data/2.5/weather?"

# Load model dan vectorizer terbaru
with open("intent_model.pkl", "rb") as f:
    model = pickle.load(f)

with open("vectorizer.pkl", "rb") as f:
    vectorizer = pickle.load(f)

# Load intents
with open("intents.json", "r", encoding="utf-8") as f:
    intents = json.load(f)

def classify_intent(text):
    vec = vectorizer.transform([text])
    return model.predict(vec)[0]

def save_conversation_log(user_input, bot_response):
    with open("conversation_log.txt", "a") as f:
        f.write(f"{datetime.datetime.now()} - User: {user_input} - Bot: {bot_response}\n")

def get_weather(city="Semarang"):
    complete_url = f"{BASE_URL}q={city}&appid={API_KEY}&units=metric"
    response = requests.get(complete_url)
    data = response.json()
    if data["cod"] != "404":
        main_data = data["main"]
        weather_data = data["weather"][0]
        return f"Suhu di {city}: {main_data['temp']}°C dengan cuaca {weather_data['description']}."
    else:
        return "Maaf, saya tidak bisa mendapatkan data cuaca saat ini."

def get_response(user_input):
    user_input = user_input.lower()

    if "cuaca" in user_input or "suhu" in user_input:
        response = get_weather()
        save_conversation_log(user_input, response)
        return response

    intent_tag = classify_intent(user_input)

    for intent in intents["intents"]:
        if intent["tag"] == intent_tag:
            response = random.choice(intent["responses"])
            save_conversation_log(user_input, response)
            return response

    fallback = random.choice([
        "Maaf, aku belum paham. Bisa dijelaskan dengan cara lain?",
        "Saya belum tahu jawabannya, tapi saya akan belajar!",
        "Coba tanya dengan kata berbeda, yuk!"
    ])
    save_conversation_log(user_input, fallback)
    return fallback

chatbot_code = """
import random
import datetime
import requests
import pickle
import json

API_KEY = "b46e6fad8459cedb9b6d6bc6b40bc4d4"
BASE_URL = "http://api.openweathermap.org/data/2.5/weather?"

with open("intent_model.pkl", "rb") as f:
    model = pickle.load(f)

with open("vectorizer.pkl", "rb") as f:
    vectorizer = pickle.load(f)

with open("intents.json", "r", encoding="utf-8") as f:
    intents = json.load(f)

def classify_intent(text):
    vec = vectorizer.transform([text])
    return model.predict(vec)[0]

def save_conversation_log(user_input, bot_response):
    with open("conversation_log.txt", "a") as f:
        f.write(f"{datetime.datetime.now()} - User: {user_input} - Bot: {bot_response}\\n")

def get_weather(city="Semarang"):
    complete_url = f"{BASE_URL}q={city}&appid={API_KEY}&units=metric"
    response = requests.get(complete_url)
    data = response.json()
    if data["cod"] != "404":
        main_data = data["main"]
        weather_data = data["weather"][0]
        return f"Suhu di {city}: {main_data['temp']}°C dengan cuaca {weather_data['description']}."
    else:
        return "Maaf, saya tidak bisa mendapatkan data cuaca saat ini."

def get_response(user_input):
    user_input = user_input.lower()
    if "cuaca" in user_input or "suhu" in user_input:
        response = get_weather()
        save_conversation_log(user_input, response)
        return response

    intent_tag = classify_intent(user_input)

    for intent in intents["intents"]:
        if intent["tag"] == intent_tag:
            response = random.choice(intent["responses"])
            save_conversation_log(user_input, response)
            return response

    fallback = random.choice([
        "Maaf, aku belum paham. Bisa dijelaskan dengan cara lain?",
        "Saya belum tahu jawabannya, tapi saya akan belajar!",
        "Coba tanya dengan kata berbeda, yuk!"
    ])
    save_conversation_log(user_input, fallback)
    return fallback
"""

with open("chatbot.py", "w", encoding="utf-8") as f:
    f.write(chatbot_code)

print("✅ chatbot.py berhasil dibuat!")

!pip install pyngrok streamlit --quiet

# Simpan app.py
with open("app.py", "w") as f:
    f.write("""
import streamlit as st
import time
from chatbot import get_response

st.set_page_config(page_title="Chatbot Cerdas", layout="centered")
st.title("🤖 Chatbot Cerdas dengan Cuaca & AI")

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

user_input = st.text_input("Kamu:", "")

if user_input:
    st.session_state.chat_history.append(("Kamu", user_input))
    with st.spinner("Bot sedang mengetik..."):
        time.sleep(1.2)
        response = get_response(user_input)
    st.session_state.chat_history.append(("Bot", response))

st.markdown("---")
st.subheader("💬 Riwayat Percakapan")
for sender, msg in st.session_state.chat_history:
    st.markdown(f"**{'🧑 Kamu' if sender == 'Kamu' else '🤖 Bot'}:** {msg}")
""")

from pyngrok import ngrok

# Autentikasi ngrok
ngrok.set_auth_token("2w8HPYtfFnX2rtpXuYdFIbOdhEh_4HF3VXkjYSbufwsovxKPM")

# Jalankan streamlit
public_url = ngrok.connect(8501)
print("🌐 Akses chatbot kamu di:", public_url)

!streamlit run app.py &>/content/log.txt &

pip install streamlit pyngrok

from pyngrok import ngrok
import os

# Jalankan streamlit di background
os.system("streamlit run app.py &")

# Buka tunnel di port 8501
public_url = ngrok.connect(8501)
print(f"🚀 Ngrok public URL: {public_url}")

python ngrok_app.py